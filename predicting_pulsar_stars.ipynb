{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pulsar_stars.csv']\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras import losses\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "dataset = pd.read_csv('pulsar_stars.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9fe739112befa8fb7895d7b1e786ceb251a07fa9"
   },
   "source": [
    "1. Once the dataset is loaded, let's see some relevant information about the dataset such as column's titles and types, number of records and shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "7b38a95fd67169cdf19f4b9730a9c1ac7b5aeece",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17898 entries, 0 to 17897\n",
      "Data columns (total 9 columns):\n",
      " Mean of the integrated profile                  17898 non-null float64\n",
      " Standard deviation of the integrated profile    17898 non-null float64\n",
      " Excess kurtosis of the integrated profile       17898 non-null float64\n",
      " Skewness of the integrated profile              17898 non-null float64\n",
      " Mean of the DM-SNR curve                        17898 non-null float64\n",
      " Standard deviation of the DM-SNR curve          17898 non-null float64\n",
      " Excess kurtosis of the DM-SNR curve             17898 non-null float64\n",
      " Skewness of the DM-SNR curve                    17898 non-null float64\n",
      "target_class                                     17898 non-null int64\n",
      "dtypes: float64(8), int64(1)\n",
      "memory usage: 1.2 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# EDA\n",
    "print(dataset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "67889e172218a90a818263626ecc3d664cffb929"
   },
   "source": [
    "2. Now, let's divide our dataset into two subsets: data and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "bd23ddabb0612244c6240871edc8c6ab8cd7a60a"
   },
   "outputs": [],
   "source": [
    "# Droping the target column\n",
    "data = dataset.drop(['target_class'], axis=1)\n",
    "\n",
    "# Normalizing the data for all data points to fit in the range [0,1]\n",
    "data = data / np.max(data)\n",
    "\n",
    "target = dataset[['target_class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "e0192aa64ca2a7d5fcb99549e21349bcb01c92aa"
   },
   "outputs": [],
   "source": [
    "# Construct the training and testing splits \n",
    "trainX, testX, trainY, testY = train_test_split(data, target, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "983a1f5f87eadaa31ce04178e37f3d4cc981088d"
   },
   "source": [
    "Machine Learning algorithms works better when integer labels are transformed into vector labels. In order to accomplish this transformation I will instantiate a LabelBinarizer object and apply the transformation methods into our trainY and testY sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "1315d227e4982c4ea529f01223a10a9475a7ad1e"
   },
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "testY = lb.transform(testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "391d1fe28b56d737665d07666428cf1fc39b6bda"
   },
   "source": [
    "With keras, it is possible to define models to our neural network (nn). In this case, we are going to work with a Sequential nn, which is just the nn as we already know, i.e., each layer has as its input the output of the former layer. It is worth mention that our neural network is 8-4-2-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "60bb73df10aa290dc09b65259e988855e4a08a6c"
   },
   "outputs": [],
   "source": [
    "# Defining the model\n",
    "model = Sequential()\n",
    "model.add(Dense(4, input_shape=(8,), activation='sigmoid'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "59eb9e91bd5c00b37dbeb580c9bc5e91f5a6cc04"
   },
   "source": [
    "We are going to use the Stochastic Gradient Descent technique as an optimizer, initially with a learning rate = 0.1. Furthermore, as this is a binary classification problem, in this case a common loss function to use is the binary cross-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "d8e991ba785b2b7e9de04a49be0dcddb461ca6f4",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13423 samples, validate on 4475 samples\n",
      "Epoch 1/200\n",
      "13423/13423 [==============================] - 1s 38us/step - loss: 0.3578 - acc: 0.9090 - val_loss: 0.3127 - val_acc: 0.9068\n",
      "Epoch 2/200\n",
      "13423/13423 [==============================] - 0s 18us/step - loss: 0.3061 - acc: 0.9090 - val_loss: 0.3100 - val_acc: 0.9068\n",
      "Epoch 3/200\n",
      "13423/13423 [==============================] - 0s 12us/step - loss: 0.3050 - acc: 0.9090 - val_loss: 0.3095 - val_acc: 0.9068\n",
      "Epoch 4/200\n",
      "13423/13423 [==============================] - 0s 12us/step - loss: 0.3045 - acc: 0.9090 - val_loss: 0.3090 - val_acc: 0.9068\n",
      "Epoch 5/200\n",
      "13423/13423 [==============================] - 0s 12us/step - loss: 0.3040 - acc: 0.9090 - val_loss: 0.3085 - val_acc: 0.9068\n",
      "Epoch 6/200\n",
      "13423/13423 [==============================] - 0s 12us/step - loss: 0.3035 - acc: 0.9090 - val_loss: 0.3080 - val_acc: 0.9068\n",
      "Epoch 7/200\n",
      "13423/13423 [==============================] - 0s 12us/step - loss: 0.3030 - acc: 0.9090 - val_loss: 0.3075 - val_acc: 0.9068\n",
      "Epoch 8/200\n",
      "13423/13423 [==============================] - 0s 12us/step - loss: 0.3024 - acc: 0.9090 - val_loss: 0.3069 - val_acc: 0.9068\n",
      "Epoch 9/200\n",
      "13423/13423 [==============================] - 0s 12us/step - loss: 0.3018 - acc: 0.9090 - val_loss: 0.3063 - val_acc: 0.9068\n",
      "Epoch 10/200\n",
      "13423/13423 [==============================] - 0s 12us/step - loss: 0.3011 - acc: 0.9090 - val_loss: 0.3055 - val_acc: 0.9068\n",
      "Epoch 11/200\n",
      "13423/13423 [==============================] - 0s 13us/step - loss: 0.3003 - acc: 0.9090 - val_loss: 0.3047 - val_acc: 0.9068\n",
      "Epoch 12/200\n",
      "13423/13423 [==============================] - 0s 15us/step - loss: 0.2994 - acc: 0.9090 - val_loss: 0.3037 - val_acc: 0.9068\n",
      "Epoch 13/200\n",
      "13423/13423 [==============================] - 0s 12us/step - loss: 0.2985 - acc: 0.9090 - val_loss: 0.3027 - val_acc: 0.9068\n",
      "Epoch 14/200\n",
      "13423/13423 [==============================] - 0s 11us/step - loss: 0.2974 - acc: 0.9090 - val_loss: 0.3015 - val_acc: 0.9068\n",
      "Epoch 15/200\n",
      "13423/13423 [==============================] - 0s 11us/step - loss: 0.2961 - acc: 0.9090 - val_loss: 0.3001 - val_acc: 0.9068\n",
      "Epoch 16/200\n",
      "13423/13423 [==============================] - 0s 11us/step - loss: 0.2946 - acc: 0.9090 - val_loss: 0.2985 - val_acc: 0.9068\n",
      "Epoch 17/200\n",
      "13423/13423 [==============================] - 0s 11us/step - loss: 0.2930 - acc: 0.9090 - val_loss: 0.2967 - val_acc: 0.9068\n",
      "Epoch 18/200\n",
      "13423/13423 [==============================] - 0s 11us/step - loss: 0.2910 - acc: 0.9090 - val_loss: 0.2946 - val_acc: 0.9068\n",
      "Epoch 19/200\n",
      "13423/13423 [==============================] - 0s 11us/step - loss: 0.2888 - acc: 0.9090 - val_loss: 0.2922 - val_acc: 0.9068\n",
      "Epoch 20/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.2862 - acc: 0.9090 - val_loss: 0.2893 - val_acc: 0.9068\n",
      "Epoch 21/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.2832 - acc: 0.9090 - val_loss: 0.2861 - val_acc: 0.9068\n",
      "Epoch 22/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.2797 - acc: 0.9090 - val_loss: 0.2823 - val_acc: 0.9068\n",
      "Epoch 23/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.2757 - acc: 0.9090 - val_loss: 0.2779 - val_acc: 0.9068\n",
      "Epoch 24/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.2711 - acc: 0.9090 - val_loss: 0.2730 - val_acc: 0.9068\n",
      "Epoch 25/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.2658 - acc: 0.9090 - val_loss: 0.2672 - val_acc: 0.9068\n",
      "Epoch 26/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.2598 - acc: 0.9090 - val_loss: 0.2607 - val_acc: 0.9068\n",
      "Epoch 27/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.2530 - acc: 0.9090 - val_loss: 0.2535 - val_acc: 0.9068\n",
      "Epoch 28/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.2455 - acc: 0.9090 - val_loss: 0.2454 - val_acc: 0.9068\n",
      "Epoch 29/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.2374 - acc: 0.9090 - val_loss: 0.2368 - val_acc: 0.9068\n",
      "Epoch 30/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.2286 - acc: 0.9090 - val_loss: 0.2275 - val_acc: 0.9068\n",
      "Epoch 31/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.2193 - acc: 0.9090 - val_loss: 0.2179 - val_acc: 0.9068\n",
      "Epoch 32/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.2098 - acc: 0.9090 - val_loss: 0.2081 - val_acc: 0.9068\n",
      "Epoch 33/200\n",
      "13423/13423 [==============================] - 0s 11us/step - loss: 0.2002 - acc: 0.9090 - val_loss: 0.1983 - val_acc: 0.9068\n",
      "Epoch 34/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.1908 - acc: 0.9090 - val_loss: 0.1887 - val_acc: 0.9068\n",
      "Epoch 35/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.1817 - acc: 0.9090 - val_loss: 0.1797 - val_acc: 0.9068\n",
      "Epoch 36/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.1731 - acc: 0.9148 - val_loss: 0.1712 - val_acc: 0.9182\n",
      "Epoch 37/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.1652 - acc: 0.9286 - val_loss: 0.1632 - val_acc: 0.9345\n",
      "Epoch 38/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.1579 - acc: 0.9406 - val_loss: 0.1560 - val_acc: 0.9437\n",
      "Epoch 39/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.1513 - acc: 0.9476 - val_loss: 0.1495 - val_acc: 0.9470\n",
      "Epoch 40/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.1455 - acc: 0.9518 - val_loss: 0.1436 - val_acc: 0.9522\n",
      "Epoch 41/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.1402 - acc: 0.9549 - val_loss: 0.1385 - val_acc: 0.9549\n",
      "Epoch 42/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.1355 - acc: 0.9569 - val_loss: 0.1337 - val_acc: 0.9566\n",
      "Epoch 43/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.1312 - acc: 0.9592 - val_loss: 0.1294 - val_acc: 0.9598\n",
      "Epoch 44/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.1274 - acc: 0.9610 - val_loss: 0.1256 - val_acc: 0.9609\n",
      "Epoch 45/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.1240 - acc: 0.9630 - val_loss: 0.1222 - val_acc: 0.9625\n",
      "Epoch 46/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.1210 - acc: 0.9644 - val_loss: 0.1190 - val_acc: 0.9638\n",
      "Epoch 47/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.1182 - acc: 0.9657 - val_loss: 0.1161 - val_acc: 0.9649\n",
      "Epoch 48/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.1155 - acc: 0.9671 - val_loss: 0.1138 - val_acc: 0.9647\n",
      "Epoch 49/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.1133 - acc: 0.9674 - val_loss: 0.1112 - val_acc: 0.9667\n",
      "Epoch 50/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.1112 - acc: 0.9684 - val_loss: 0.1090 - val_acc: 0.9674\n",
      "Epoch 51/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.1093 - acc: 0.9688 - val_loss: 0.1070 - val_acc: 0.9676\n",
      "Epoch 52/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.1075 - acc: 0.9691 - val_loss: 0.1051 - val_acc: 0.9692\n",
      "Epoch 53/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.1058 - acc: 0.9697 - val_loss: 0.1034 - val_acc: 0.9696\n",
      "Epoch 54/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.1043 - acc: 0.9706 - val_loss: 0.1017 - val_acc: 0.9709\n",
      "Epoch 55/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.1029 - acc: 0.9707 - val_loss: 0.1002 - val_acc: 0.9718\n",
      "Epoch 56/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.1015 - acc: 0.9707 - val_loss: 0.0989 - val_acc: 0.9723\n",
      "Epoch 57/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.1003 - acc: 0.9713 - val_loss: 0.0977 - val_acc: 0.9721\n",
      "Epoch 58/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0992 - acc: 0.9715 - val_loss: 0.0964 - val_acc: 0.9727\n",
      "Epoch 59/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0981 - acc: 0.9715 - val_loss: 0.0953 - val_acc: 0.9736\n",
      "Epoch 60/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0972 - acc: 0.9715 - val_loss: 0.0942 - val_acc: 0.9736\n",
      "Epoch 61/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0962 - acc: 0.9720 - val_loss: 0.0933 - val_acc: 0.9741\n",
      "Epoch 62/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0954 - acc: 0.9719 - val_loss: 0.0924 - val_acc: 0.9741\n",
      "Epoch 63/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0946 - acc: 0.9723 - val_loss: 0.0915 - val_acc: 0.9747\n",
      "Epoch 64/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0939 - acc: 0.9725 - val_loss: 0.0907 - val_acc: 0.9743\n",
      "Epoch 65/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0931 - acc: 0.9730 - val_loss: 0.0901 - val_acc: 0.9747\n",
      "Epoch 66/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0925 - acc: 0.9728 - val_loss: 0.0894 - val_acc: 0.9752\n",
      "Epoch 67/200\n",
      "13423/13423 [==============================] - 0s 11us/step - loss: 0.0920 - acc: 0.9733 - val_loss: 0.0887 - val_acc: 0.9754\n",
      "Epoch 68/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0914 - acc: 0.9735 - val_loss: 0.0881 - val_acc: 0.9756\n",
      "Epoch 69/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0909 - acc: 0.9739 - val_loss: 0.0875 - val_acc: 0.9754\n",
      "Epoch 70/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0904 - acc: 0.9743 - val_loss: 0.0870 - val_acc: 0.9754\n",
      "Epoch 71/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0899 - acc: 0.9744 - val_loss: 0.0865 - val_acc: 0.9754\n",
      "Epoch 72/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0895 - acc: 0.9740 - val_loss: 0.0860 - val_acc: 0.9756\n",
      "Epoch 73/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0891 - acc: 0.9745 - val_loss: 0.0856 - val_acc: 0.9756\n",
      "Epoch 74/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0887 - acc: 0.9745 - val_loss: 0.0853 - val_acc: 0.9756\n",
      "Epoch 75/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0884 - acc: 0.9742 - val_loss: 0.0848 - val_acc: 0.9756\n",
      "Epoch 76/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0881 - acc: 0.9747 - val_loss: 0.0844 - val_acc: 0.9759\n",
      "Epoch 77/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0878 - acc: 0.9747 - val_loss: 0.0841 - val_acc: 0.9763\n",
      "Epoch 78/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0875 - acc: 0.9751 - val_loss: 0.0838 - val_acc: 0.9765\n",
      "Epoch 79/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0872 - acc: 0.9749 - val_loss: 0.0834 - val_acc: 0.9765\n",
      "Epoch 80/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0868 - acc: 0.9750 - val_loss: 0.0835 - val_acc: 0.9772\n",
      "Epoch 81/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0867 - acc: 0.9758 - val_loss: 0.0829 - val_acc: 0.9765\n",
      "Epoch 82/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0865 - acc: 0.9751 - val_loss: 0.0827 - val_acc: 0.9765\n",
      "Epoch 83/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0862 - acc: 0.9753 - val_loss: 0.0824 - val_acc: 0.9765\n",
      "Epoch 84/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0861 - acc: 0.9754 - val_loss: 0.0821 - val_acc: 0.9765\n",
      "Epoch 85/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0859 - acc: 0.9753 - val_loss: 0.0819 - val_acc: 0.9770\n",
      "Epoch 86/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0857 - acc: 0.9757 - val_loss: 0.0817 - val_acc: 0.9765\n",
      "Epoch 87/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0856 - acc: 0.9753 - val_loss: 0.0815 - val_acc: 0.9768\n",
      "Epoch 88/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0854 - acc: 0.9755 - val_loss: 0.0813 - val_acc: 0.9772\n",
      "Epoch 89/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0852 - acc: 0.9760 - val_loss: 0.0811 - val_acc: 0.9768\n",
      "Epoch 90/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0851 - acc: 0.9758 - val_loss: 0.0809 - val_acc: 0.9772\n",
      "Epoch 91/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0850 - acc: 0.9757 - val_loss: 0.0808 - val_acc: 0.9774\n",
      "Epoch 92/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0848 - acc: 0.9757 - val_loss: 0.0806 - val_acc: 0.9772\n",
      "Epoch 93/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0847 - acc: 0.9757 - val_loss: 0.0804 - val_acc: 0.9772\n",
      "Epoch 94/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0846 - acc: 0.9757 - val_loss: 0.0803 - val_acc: 0.9772\n",
      "Epoch 95/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0845 - acc: 0.9757 - val_loss: 0.0802 - val_acc: 0.9777\n",
      "Epoch 96/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0844 - acc: 0.9757 - val_loss: 0.0801 - val_acc: 0.9777\n",
      "Epoch 97/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0842 - acc: 0.9762 - val_loss: 0.0799 - val_acc: 0.9770\n",
      "Epoch 98/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0841 - acc: 0.9757 - val_loss: 0.0798 - val_acc: 0.9772\n",
      "Epoch 99/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0841 - acc: 0.9758 - val_loss: 0.0796 - val_acc: 0.9777\n",
      "Epoch 100/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0840 - acc: 0.9757 - val_loss: 0.0795 - val_acc: 0.9777\n",
      "Epoch 101/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0839 - acc: 0.9759 - val_loss: 0.0794 - val_acc: 0.9777\n",
      "Epoch 102/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0838 - acc: 0.9759 - val_loss: 0.0793 - val_acc: 0.9777\n",
      "Epoch 103/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0837 - acc: 0.9757 - val_loss: 0.0792 - val_acc: 0.9777\n",
      "Epoch 104/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0836 - acc: 0.9761 - val_loss: 0.0791 - val_acc: 0.9777\n",
      "Epoch 105/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0836 - acc: 0.9760 - val_loss: 0.0790 - val_acc: 0.9777\n",
      "Epoch 106/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0835 - acc: 0.9762 - val_loss: 0.0789 - val_acc: 0.9777\n",
      "Epoch 107/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0834 - acc: 0.9760 - val_loss: 0.0788 - val_acc: 0.9777\n",
      "Epoch 108/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0833 - acc: 0.9762 - val_loss: 0.0787 - val_acc: 0.9777\n",
      "Epoch 109/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0832 - acc: 0.9758 - val_loss: 0.0787 - val_acc: 0.9777\n",
      "Epoch 110/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0832 - acc: 0.9762 - val_loss: 0.0785 - val_acc: 0.9779\n",
      "Epoch 111/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0830 - acc: 0.9759 - val_loss: 0.0785 - val_acc: 0.9777\n",
      "Epoch 112/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0830 - acc: 0.9765 - val_loss: 0.0784 - val_acc: 0.9781\n",
      "Epoch 113/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0830 - acc: 0.9759 - val_loss: 0.0783 - val_acc: 0.9777\n",
      "Epoch 114/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0829 - acc: 0.9766 - val_loss: 0.0782 - val_acc: 0.9777\n",
      "Epoch 115/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0829 - acc: 0.9764 - val_loss: 0.0781 - val_acc: 0.9781\n",
      "Epoch 116/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0828 - acc: 0.9762 - val_loss: 0.0780 - val_acc: 0.9781\n",
      "Epoch 117/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0827 - acc: 0.9765 - val_loss: 0.0779 - val_acc: 0.9781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0827 - acc: 0.9762 - val_loss: 0.0779 - val_acc: 0.9781\n",
      "Epoch 119/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0826 - acc: 0.9764 - val_loss: 0.0778 - val_acc: 0.9781\n",
      "Epoch 120/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0826 - acc: 0.9761 - val_loss: 0.0777 - val_acc: 0.9781\n",
      "Epoch 121/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0825 - acc: 0.9762 - val_loss: 0.0777 - val_acc: 0.9781\n",
      "Epoch 122/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0825 - acc: 0.9765 - val_loss: 0.0776 - val_acc: 0.9781\n",
      "Epoch 123/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0824 - acc: 0.9766 - val_loss: 0.0775 - val_acc: 0.9781\n",
      "Epoch 124/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0824 - acc: 0.9764 - val_loss: 0.0775 - val_acc: 0.9781\n",
      "Epoch 125/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0824 - acc: 0.9762 - val_loss: 0.0774 - val_acc: 0.9781\n",
      "Epoch 126/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0823 - acc: 0.9765 - val_loss: 0.0773 - val_acc: 0.9781\n",
      "Epoch 127/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0823 - acc: 0.9765 - val_loss: 0.0773 - val_acc: 0.9781\n",
      "Epoch 128/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0822 - acc: 0.9763 - val_loss: 0.0774 - val_acc: 0.9781\n",
      "Epoch 129/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0822 - acc: 0.9765 - val_loss: 0.0771 - val_acc: 0.9781\n",
      "Epoch 130/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0821 - acc: 0.9765 - val_loss: 0.0771 - val_acc: 0.9781\n",
      "Epoch 131/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0820 - acc: 0.9764 - val_loss: 0.0771 - val_acc: 0.9783\n",
      "Epoch 132/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0820 - acc: 0.9767 - val_loss: 0.0770 - val_acc: 0.9783\n",
      "Epoch 133/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0819 - acc: 0.9762 - val_loss: 0.0769 - val_acc: 0.9781\n",
      "Epoch 134/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0819 - acc: 0.9763 - val_loss: 0.0770 - val_acc: 0.9783\n",
      "Epoch 135/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0819 - acc: 0.9768 - val_loss: 0.0769 - val_acc: 0.9783\n",
      "Epoch 136/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0819 - acc: 0.9766 - val_loss: 0.0769 - val_acc: 0.9783\n",
      "Epoch 137/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0818 - acc: 0.9766 - val_loss: 0.0767 - val_acc: 0.9783\n",
      "Epoch 138/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0818 - acc: 0.9768 - val_loss: 0.0766 - val_acc: 0.9783\n",
      "Epoch 139/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0817 - acc: 0.9767 - val_loss: 0.0766 - val_acc: 0.9783\n",
      "Epoch 140/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0816 - acc: 0.9767 - val_loss: 0.0768 - val_acc: 0.9785\n",
      "Epoch 141/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0817 - acc: 0.9765 - val_loss: 0.0765 - val_acc: 0.9783\n",
      "Epoch 142/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0816 - acc: 0.9765 - val_loss: 0.0764 - val_acc: 0.9785\n",
      "Epoch 143/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0815 - acc: 0.9766 - val_loss: 0.0764 - val_acc: 0.9783\n",
      "Epoch 144/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0815 - acc: 0.9766 - val_loss: 0.0763 - val_acc: 0.9785\n",
      "Epoch 145/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0814 - acc: 0.9765 - val_loss: 0.0763 - val_acc: 0.9783\n",
      "Epoch 146/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0815 - acc: 0.9768 - val_loss: 0.0762 - val_acc: 0.9781\n",
      "Epoch 147/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0814 - acc: 0.9765 - val_loss: 0.0761 - val_acc: 0.9785\n",
      "Epoch 148/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0814 - acc: 0.9768 - val_loss: 0.0761 - val_acc: 0.9785\n",
      "Epoch 149/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0813 - acc: 0.9768 - val_loss: 0.0761 - val_acc: 0.9781\n",
      "Epoch 150/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0813 - acc: 0.9767 - val_loss: 0.0760 - val_acc: 0.9783\n",
      "Epoch 151/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0813 - acc: 0.9766 - val_loss: 0.0759 - val_acc: 0.9785\n",
      "Epoch 152/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0812 - acc: 0.9768 - val_loss: 0.0759 - val_acc: 0.9783\n",
      "Epoch 153/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0812 - acc: 0.9768 - val_loss: 0.0759 - val_acc: 0.9785\n",
      "Epoch 154/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0811 - acc: 0.9770 - val_loss: 0.0758 - val_acc: 0.9785\n",
      "Epoch 155/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0811 - acc: 0.9768 - val_loss: 0.0758 - val_acc: 0.9783\n",
      "Epoch 156/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0811 - acc: 0.9770 - val_loss: 0.0757 - val_acc: 0.9783\n",
      "Epoch 157/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0810 - acc: 0.9768 - val_loss: 0.0758 - val_acc: 0.9788\n",
      "Epoch 158/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0811 - acc: 0.9768 - val_loss: 0.0757 - val_acc: 0.9785\n",
      "Epoch 159/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0809 - acc: 0.9768 - val_loss: 0.0756 - val_acc: 0.9785\n",
      "Epoch 160/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0809 - acc: 0.9768 - val_loss: 0.0757 - val_acc: 0.9788\n",
      "Epoch 161/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0809 - acc: 0.9768 - val_loss: 0.0755 - val_acc: 0.9785\n",
      "Epoch 162/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0808 - acc: 0.9769 - val_loss: 0.0755 - val_acc: 0.9788\n",
      "Epoch 163/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0808 - acc: 0.9768 - val_loss: 0.0754 - val_acc: 0.9783\n",
      "Epoch 164/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0808 - acc: 0.9767 - val_loss: 0.0754 - val_acc: 0.9785\n",
      "Epoch 165/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0807 - acc: 0.9765 - val_loss: 0.0753 - val_acc: 0.9788\n",
      "Epoch 166/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0807 - acc: 0.9769 - val_loss: 0.0754 - val_acc: 0.9790\n",
      "Epoch 167/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0806 - acc: 0.9770 - val_loss: 0.0753 - val_acc: 0.9788\n",
      "Epoch 168/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0806 - acc: 0.9766 - val_loss: 0.0752 - val_acc: 0.9785\n",
      "Epoch 169/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0807 - acc: 0.9768 - val_loss: 0.0751 - val_acc: 0.9785\n",
      "Epoch 170/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0806 - acc: 0.9768 - val_loss: 0.0751 - val_acc: 0.9788\n",
      "Epoch 171/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0806 - acc: 0.9768 - val_loss: 0.0752 - val_acc: 0.9790\n",
      "Epoch 172/200\n",
      "13423/13423 [==============================] - 0s 11us/step - loss: 0.0806 - acc: 0.9768 - val_loss: 0.0750 - val_acc: 0.9785\n",
      "Epoch 173/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0805 - acc: 0.9768 - val_loss: 0.0750 - val_acc: 0.9788\n",
      "Epoch 174/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0805 - acc: 0.9770 - val_loss: 0.0750 - val_acc: 0.9790\n",
      "Epoch 175/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0805 - acc: 0.9771 - val_loss: 0.0749 - val_acc: 0.9785\n",
      "Epoch 176/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0805 - acc: 0.9770 - val_loss: 0.0748 - val_acc: 0.9785\n",
      "Epoch 177/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0803 - acc: 0.9768 - val_loss: 0.0750 - val_acc: 0.9790\n",
      "Epoch 178/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0803 - acc: 0.9770 - val_loss: 0.0748 - val_acc: 0.9785\n",
      "Epoch 179/200\n",
      "13423/13423 [==============================] - 0s 11us/step - loss: 0.0803 - acc: 0.9769 - val_loss: 0.0747 - val_acc: 0.9785\n",
      "Epoch 180/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0802 - acc: 0.9771 - val_loss: 0.0749 - val_acc: 0.9785\n",
      "Epoch 181/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0803 - acc: 0.9769 - val_loss: 0.0747 - val_acc: 0.9792\n",
      "Epoch 182/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0802 - acc: 0.9771 - val_loss: 0.0746 - val_acc: 0.9792\n",
      "Epoch 183/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0803 - acc: 0.9768 - val_loss: 0.0746 - val_acc: 0.9792\n",
      "Epoch 184/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0801 - acc: 0.9771 - val_loss: 0.0745 - val_acc: 0.9788\n",
      "Epoch 185/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0802 - acc: 0.9769 - val_loss: 0.0745 - val_acc: 0.9788\n",
      "Epoch 186/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0801 - acc: 0.9770 - val_loss: 0.0745 - val_acc: 0.9788\n",
      "Epoch 187/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0801 - acc: 0.9769 - val_loss: 0.0744 - val_acc: 0.9792\n",
      "Epoch 188/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0800 - acc: 0.9771 - val_loss: 0.0744 - val_acc: 0.9788\n",
      "Epoch 189/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0800 - acc: 0.9769 - val_loss: 0.0743 - val_acc: 0.9785\n",
      "Epoch 190/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0800 - acc: 0.9770 - val_loss: 0.0743 - val_acc: 0.9792\n",
      "Epoch 191/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0800 - acc: 0.9769 - val_loss: 0.0743 - val_acc: 0.9792\n",
      "Epoch 192/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0799 - acc: 0.9772 - val_loss: 0.0742 - val_acc: 0.9792\n",
      "Epoch 193/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0799 - acc: 0.9773 - val_loss: 0.0742 - val_acc: 0.9792\n",
      "Epoch 194/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0799 - acc: 0.9771 - val_loss: 0.0742 - val_acc: 0.9792\n",
      "Epoch 195/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0798 - acc: 0.9772 - val_loss: 0.0742 - val_acc: 0.9792\n",
      "Epoch 196/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0798 - acc: 0.9772 - val_loss: 0.0744 - val_acc: 0.9792\n",
      "Epoch 197/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0798 - acc: 0.9768 - val_loss: 0.0741 - val_acc: 0.9792\n",
      "Epoch 198/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0798 - acc: 0.9769 - val_loss: 0.0740 - val_acc: 0.9792\n",
      "Epoch 199/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0796 - acc: 0.9769 - val_loss: 0.0740 - val_acc: 0.9790\n",
      "Epoch 200/200\n",
      "13423/13423 [==============================] - 0s 10us/step - loss: 0.0797 - acc: 0.9770 - val_loss: 0.0740 - val_acc: 0.9792\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient descent optimizer with learning rate = 0.1\n",
    "sgd = SGD(0.1)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd,\n",
    "    metrics=[\"accuracy\"])\n",
    "H = model.fit(trainX, trainY, validation_data=(testX, testY), batch_size=128, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "1de109def43c2d85aa686fe759a1980957cd6988",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Prediction       1.00      1.00      1.00      4475\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      4475\n",
      "   macro avg       1.00      1.00      1.00      4475\n",
      "weighted avg       1.00      1.00      1.00      4475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(testX, batch_size=128)\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "    predictions.argmax(axis=1),\n",
    "    target_names=['Prediction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "d1611f860f5b1b2c370da63371a3ff2896ae8626"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the curve Epoch vs. Loss/Accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, 200), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, 200), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, 200), H.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, 200), H.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig('plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "db6dc8908975dcaea032c7e28a4662644bd5513c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
